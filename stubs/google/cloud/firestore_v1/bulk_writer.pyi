import datetime
import enum
from _typeshed import Incomplete
from dataclasses import dataclass
from google.cloud.firestore_v1 import _helpers
from google.cloud.firestore_v1.base_client import BaseClient as BaseClient
from google.cloud.firestore_v1.base_document import BaseDocumentReference as BaseDocumentReference
from google.cloud.firestore_v1.bulk_batch import BulkWriteBatch
from google.cloud.firestore_v1.types.firestore import BatchWriteResponse as BatchWriteResponse
from google.cloud.firestore_v1.types.write import WriteResult as WriteResult
from google.rpc import status_pb2 as status_pb2
from typing import Callable

logger: Incomplete

class BulkRetry(enum.Enum):
    exponential = ...
    linear = ...
    immediate = ...

class SendMode(enum.Enum):
    parallel = ...
    serial = ...

class AsyncBulkWriterMixin: ...

class BulkWriter(AsyncBulkWriterMixin):
    batch_size: int
    def __init__(self, client: BaseClient | None = None, options: BulkWriterOptions | None = None) -> None: ...
    def flush(self) -> None: ...
    def close(self) -> None: ...
    def create(self, reference: BaseDocumentReference, document_data: dict, attempts: int = 0) -> None: ...
    def delete(self, reference: BaseDocumentReference, option: _helpers.WriteOption | None = None, attempts: int = 0) -> None: ...
    def set(self, reference: BaseDocumentReference, document_data: dict, merge: bool | list = False, attempts: int = 0) -> None: ...
    def update(self, reference: BaseDocumentReference, field_updates: dict, option: _helpers.WriteOption | None = None, attempts: int = 0) -> None: ...
    def on_write_result(self, callback: Callable[[BaseDocumentReference, WriteResult, BulkWriter], None] | None) -> None: ...
    def on_batch_result(self, callback: Callable[[BulkWriteBatch, BatchWriteResponse, BulkWriter], None] | None) -> None: ...
    def on_write_error(self, callback: Callable[[BulkWriteFailure, BulkWriter], bool] | None) -> None: ...

class BulkWriterOperation:
    attempts: Incomplete
    def __init__(self, attempts: int = 0) -> None: ...
    def add_to_batch(self, batch: BulkWriteBatch): ...

class BaseOperationRetry:
    def __lt__(self, other: OperationRetry): ...
    def retry(self, bulk_writer: BulkWriter) -> None: ...
    def __ge__(self, other): ...
    def __le__(self, other): ...
    def __gt__(self, other): ...

@dataclass
class BulkWriterOptions:
    initial_ops_per_second: int = ...
    max_ops_per_second: int = ...
    mode: SendMode = ...
    retry: BulkRetry = ...
    def __init__(self, initial_ops_per_second=..., max_ops_per_second=..., mode=..., retry=...) -> None: ...

@dataclass
class BulkWriteFailure:
    operation: BulkWriterOperation
    code: int
    message: str
    @property
    def attempts(self) -> int: ...
    def __init__(self, operation, code, message) -> None: ...

@dataclass
class OperationRetry(BaseOperationRetry):
    operation: BulkWriterOperation
    run_at: datetime.datetime
    def __init__(self, operation, run_at) -> None: ...

@dataclass
class BulkWriterCreateOperation(BulkWriterOperation):
    reference: BaseDocumentReference
    document_data: dict
    attempts: int = ...
    def __init__(self, reference, document_data, attempts=...) -> None: ...

@dataclass
class BulkWriterUpdateOperation(BulkWriterOperation):
    reference: BaseDocumentReference
    field_updates: dict
    option: _helpers.WriteOption | None
    attempts: int = ...
    def __init__(self, reference, field_updates, option, attempts=...) -> None: ...

@dataclass
class BulkWriterSetOperation(BulkWriterOperation):
    reference: BaseDocumentReference
    document_data: dict
    merge: bool | list = ...
    attempts: int = ...
    def __init__(self, reference, document_data, merge=..., attempts=...) -> None: ...

@dataclass
class BulkWriterDeleteOperation(BulkWriterOperation):
    reference: BaseDocumentReference
    option: _helpers.WriteOption | None
    attempts: int = ...
    def __init__(self, reference, option, attempts=...) -> None: ...
